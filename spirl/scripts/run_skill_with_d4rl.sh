#!/usr/bin/env bash
set -euo pipefail

# ===========================================
# Usage:
#   ./scripts/run_spirl_with_d4rl.sh <TASK> <D4RL_ENV> <EXP_NAME>
#
# Examples:
#   ./scripts/run_spirl_with_d4rl.sh kitchen kitchen-mixed-v0 exp_kitchen_mixed
#   ./scripts/run_spirl_with_d4rl.sh kitchen kitchen-partial-v0 exp_kitchen_partial
#   ./scripts/run_spirl_with_d4rl.sh maze    maze2d-umaze-v1   exp_maze_umaze
#   ./scripts/run_spirl_with_d4rl.sh office  antmaze-umaze-v0  exp_office_umaze
#
# Notes:
#   - TASK ∈ {kitchen, maze, office, block_stacking}
#     * office 没有 D4RL 原生数据，这里用 antmaze-* 作为 general data。
#     * block_stacking 没有 D4RL，一般只能 expert-only（本脚本也能跑，但会跳过 general）。
#   - D4RL_ENV 请用 d4rl 的合法环境名；示例见上面。
# ===========================================

TASK=${1:-kitchen}
D4RL_ENV=${2:-kitchen-mixed-v0}
EXP_NAME=${3:-serla_${TASK}}

# -------- Paths / Environment --------
export EXP_DIR=${EXP_DIR:-"$PWD/exp"}                     # SPiRL 输出根目录
export DATA_DIR=${DATA_DIR:-"$PWD/data_d4rl_general"}                   # SPiRL 数据目录
export CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}     # GPU 选择（或注释掉走默认）
GENERAL_DIR=${GENERAL_DIR:-"$PWD/data_d4rl_general/${TASK}/${D4RL_ENV}"}
D4RL_CACHE=${D4RL_CACHE:-"$HOME/.d4rl/datasets"}

# Optional: run wandb in offline mode (set to "true" to enable)
WANDB_OFFLINE=${WANDB_OFFLINE:-false}
if [[ "${WANDB_OFFLINE}" == "true" ]]; then
  export WANDB_MODE=offline
  echo "[*] W&B offline mode enabled."
fi

# Optional switches
SKIP_ONLINE=${SKIP_ONLINE:-false}         # true → 只训练 skill，不跑在线 RL
VAL_SIZE=${VAL_SIZE:-512}                 # 先验训练的验证集 batch 数
SKIP_FIRST_VAL=${SKIP_FIRST_VAL:-1}       # 1=跳过第一次 val 以加速启动
# PU/SDE hyperparams (you can override via env)
PU_LAMBDA=${PU_LAMBDA:-0.6}
PU_XI=${PU_XI:-0.0}
PU_WEIGHT=${PU_WEIGHT:-1.0}
PU_DISC_H1=${PU_DISC_H1:-256}
PU_DISC_H2=${PU_DISC_H2:-256}
SDE_SIGMA=${SDE_SIGMA:-0.01}
SDE_ALPHA=${SDE_ALPHA:-1.0}

echo "[INFO] TASK=${TASK}"
echo "[INFO] D4RL_ENV=${D4RL_ENV}"
echo "[INFO] EXP_NAME=${EXP_NAME}"
echo "[INFO] EXP_DIR=${EXP_DIR}"
echo "[INFO] GENERAL_DIR=${GENERAL_DIR}"
echo "[INFO] D4RL_CACHE=${D4RL_CACHE}"
echo "[INFO] CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}"

# -------- Basic dependency sanity check --------
python - <<'PY'
import importlib, sys
for m in ["torch","h5py","numpy","gym","d4rl"]:
    try: importlib.import_module(m)
    except Exception as e:
        print(f"[ERROR] Missing python package: {m} -> {e}"); sys.exit(1)
print("[OK] Basic python deps found.")
PY

# -------- Converter availability --------
CONVERTER="tools/convert_d4rl_hdf5_to_spirl.py"
if [[ ! -f "${CONVERTER}" ]]; then
  echo "[ERROR] Missing converter: ${CONVERTER}"
  exit 1
fi

# -------- If needed, auto-download D4RL .hdf5 and convert to SPiRL format --------
NEED_GENERAL=true
if [[ "${TASK}" == "block_stacking" ]]; then
  echo "[WARN] TASK=block_stacking has no D4RL general data; running expert-only."
  NEED_GENERAL=false
fi

if ${NEED_GENERAL}; then
  if [[ ! -d "${GENERAL_DIR}/train" ]]; then
    SRC_H5="${D4RL_CACHE}/${D4RL_ENV}.hdf5"
    if [[ ! -f "${SRC_H5}" ]]; then
      echo "[*] D4RL source not found: ${SRC_H5}"
      echo "[*] Trying to auto-download via d4rl..."
      python - <<PY
import gym, d4rl
env = gym.make("${D4RL_ENV}")
_ = d4rl.qlearning_dataset(env)   # trigger download & cache
print("[OK] Downloaded:", "${D4RL_ENV}")
PY
    fi

    if [[ ! -f "${SRC_H5}" ]]; then
      echo "[ERROR] Still no HDF5 at: ${SRC_H5}"
      echo "  - Check your D4RL_ENV='${D4RL_ENV}'"
      echo "  - Ensure d4rl is properly installed and reachable"
      exit 1
    fi

    echo "[*] Converting D4RL(HDF5) -> SPiRL HDF5: ${D4RL_ENV} -> ${GENERAL_DIR}"
    python "${CONVERTER}" --env "${D4RL_ENV}" --out "${GENERAL_DIR}"
  else
    echo "[OK] Found converted SPiRL dataset at ${GENERAL_DIR}"
  fi
fi

# -------- Write global PU defaults merged by spirl/train.py:get_config --------
PU_DEFAULTS="configs/skill_prior_learning/pu_defaults.py"
mkdir -p "$(dirname "${PU_DEFAULTS}")"
cat > "${PU_DEFAULTS}" <<EOF
# Auto-generated by run_spirl_with_d4rl.sh
pu_defaults = dict(
    expert_root = None,                 # fallback to conf.general.data_dir (SPiRL expert)
    general_root = r'${NEED_GENERAL:+${GENERAL_DIR}}',   # converted D4RL folder (or None)

    pu_lambda = ${PU_LAMBDA},
    pu_xi     = ${PU_XI},
    pu_weight = ${PU_WEIGHT},
    disc_hidden = (${PU_DISC_H1}, ${PU_DISC_H2}),

    sde_sigma_prior = ${SDE_SIGMA},
    sde_alpha       = ${SDE_ALPHA},
)
EOF
echo "[*] Wrote PU defaults: ${PU_DEFAULTS}"

# -------- Helper: pick first existing config dir that contains conf.py --------
pick_conf_dir() {
  local -n out=$1; shift
  for cand in "$@"; do
    if [[ -d "$cand" && -f "$cand/conf.py" ]]; then
      out="$cand"
      return 0
    fi
  done
  return 1
}

# -------- Pick SKILL PRIOR config (hierarchical_cl > hierarchical > flat) --------
SP_CONF=""
pick_conf_dir SP_CONF \
  "configs/skill_prior_learning/${TASK}/hierarchical_cl" \
  "configs/skill_prior_learning/${TASK}/hierarchical" \
  "configs/skill_prior_learning/${TASK}/flat" \
  "spirl/configs/skill_prior_learning/${TASK}/hierarchical_cl" \
  "spirl/configs/skill_prior_learning/${TASK}/hierarchical" \
  "spirl/configs/skill_prior_learning/${TASK}/flat" || {
    echo "[ERROR] Could not find a skill-prior config for TASK=${TASK}."
    echo "        Tried (…/hierarchical_cl, …/hierarchical, …/flat) under configs/ and spirl/configs/."
    exit 1
  }
echo "[*] Using skill-prior config: ${SP_CONF}"

# -------- Train skill prior --------
echo "[*] Training skill prior: expert=SPiRL default, general=${NEED_GENERAL:+${D4RL_ENV}}"
python -m spirl.train --path "${SP_CONF}" \
  --prefix "${EXP_NAME}" \
  --skip_first_val ${SKIP_FIRST_VAL} \
  --val_data_size ${VAL_SIZE}

# -------- Find latest checkpoint --------
SP_SUBDIR="$(basename "$(dirname "${SP_CONF}")")"   # hierarchical_cl | hierarchical | flat
CKPT_DIR="${EXP_DIR}/skill_prior_learning/${TASK}/${SP_SUBDIR}/${EXP_NAME}/weights"
if [[ ! -d "${CKPT_DIR}" ]]; then
  echo "[ERROR] Weights directory not found: ${CKPT_DIR}"
  exit 1
fi
EPOCH=$(ls "${CKPT_DIR}" | grep -Eo '^[0-9]+' | sort -n | tail -1 || true)
if [[ -z "${EPOCH}" ]]; then
  echo "[ERROR] No checkpoint found in: ${CKPT_DIR}"
  exit 1
fi
echo "[*] Latest prior checkpoint epoch: ${EPOCH}"

# -------- Optionally launch online RL (HRL + prior) --------
if [[ "${SKIP_ONLINE}" == "true" ]]; then
  echo "[OK] Skill prior training finished. (SKIP_ONLINE=true)"
  exit 0
fi

RL_CONF=""
pick_conf_dir RL_CONF \
  "configs/hrl/${TASK}/spirl_cl" \
  "configs/hrl/${TASK}/spirl" \
  "spirl/configs/hrl/${TASK}/spirl_cl" \
  "spirl/configs/hrl/${TASK}/spirl" || {
    echo "[ERROR] Could not find an RL config for TASK=${TASK} (spirl_cl/spirl)."
    exit 1
  }
echo "[*] Using RL config: ${RL_CONF}"

echo "[*] Launching online training (HRL + prior)"
python -m spirl.rl.train --path "${RL_CONF}" --resume "${EPOCH}" --prefix "${EXP_NAME}_rl"

echo "[OK] Done."

